{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "JUPYTER 환경에서 진행하였습니다. 코드상의 파일 경로가 수정되어서 진행되었습니다.\n",
    "===="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import re\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\yoons\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yoons\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       No I'm in the same boat. Still here at my moms...\n",
       "1       (Bank of Granite issues Strong-Buy) EXPLOSIVE ...\n",
       "2          They r giving a second chance to rahul dengra.\n",
       "3          O i played smash bros  &lt;#&gt;  religiously.\n",
       "4       PRIVATE! Your 2003 Account Statement for 07973...\n",
       "                              ...                        \n",
       "4452    I came hostel. I m going to sleep. Plz call me...\n",
       "4453                               Sorry, I'll call later\n",
       "4454        Prabha..i'm soryda..realy..frm heart i'm sory\n",
       "4455                           Nt joking seriously i told\n",
       "4456                  In work now. Going have in few min.\n",
       "Name: Data, Length: 4457, dtype: object"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train = pd.read_csv(\"train.csv\",encoding=\"latin-1\")\n",
    "df_test = pd.read_csv(\"test.csv\",encoding=\"latin-1\")\n",
    "df_train.Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df_train[\"Data\"]\n",
    "y_train=df_train[\"Class\"]\n",
    "X_test=df_test[\"Data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# [Empty Module #1] 텍스트 데이터 전처리\n",
    "# ------------------------------------------------\n",
    "from nltk.corpus import stopwords \n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "def data_processing(text):\n",
    "    # ------------------------------------------------------------\n",
    "    # 구현 가이드라인 \n",
    "    # ------------------------------------------------------------\n",
    "    # [1] re.sub 사용해 text 속 '[^A-Za-z]' 외의 문자만을 찾아내 제거한후, pre_words 변수에 저장\n",
    "    #      1) pattern은 '[^A-Za-z]', repl=' ' 로 각각 설정.\n",
    "    #      2) 이모지나 숫자,점과 같은 문자외의 것들을 제거했다. (이모지는 감정 분석과 관련해서 몇가지 의미를 나타내지만 이 테스크에서는 이러한 의미도 제거함.)\n",
    "    #\n",
    "    # [2] pre_words의 lower 내장 함수를 이용해 대문자들은 소문자로 변경\n",
    "    #      1)  대, 소문자가 구분되어 있으면 \"Go\"와 \"go\" 와 같이 동일한 단어를 머신은 다른 단어로 취급한다. 따라서 대문자를 모두 소문자로 변경.\n",
    "    #\n",
    "    # [3] word_tokenize 함수를 이용해 pre_word 를 토큰화하여 word를 리스트화한 후 tokenized_words변수에 저장\n",
    "    #\n",
    "    # [4] nltk 라이브러리로 다운 받은 stopwords의 \"words\" 내장 함수를 이용해 english 불용어를 찾아서 stops 변수에 저장  \n",
    "    #      1) 불용어: 텍스트 분류에서 불용어는 텍스트의 중요도을 결정하는데 영향을 미치지 않는 단어임. \n",
    "    #                    (ex: the, we, a , will), 따라서 불필요한 단어가 예측 모델에 악영향을 끼칠 수 있기 때문에 제거.\n",
    "    #\n",
    "    # [5] [3] 에서 찾은 문자열 중 단어가 [4] 에서 찾은 불용어 속에 없을 경우, tokenized_words_remove 리스트에 append \n",
    "    #\n",
    "    # [6] PorterStemmer 속 stem 내장 함수를 이용해 동일 의미를 갖는 단어를 동일한 단어로 변경하는 과정을 거친 후 다시 저장.\n",
    "    #    \n",
    "    # ------------------------------------------------------------\n",
    "    ##############\n",
    "    \n",
    "    # [1]\n",
    "    pre_words =re.sub('[^A-Za-z]', ' ', text) #문자가 아닌 경우 제거\n",
    "    ##############\n",
    "   \n",
    "    # [2]\n",
    "    pre_words = pre_words.lower() #대문자를 소문자로 전환\n",
    "    ##############\n",
    "    \n",
    "    # [3]\n",
    "    tokenized_words = word_tokenize(pre_words) #토큰화 과정을 거침\n",
    "    ##############\n",
    "    \n",
    "    # [4]\n",
    "    stops =stopwords.words(\"english\") #영어 불용어 집합을 가져와서 저장\n",
    "    new_stopsword = [\n",
    "    \"a\", \"about\", \"above\", \"across\", \"after\", \"afterwards\", \"again\", \"against\",\n",
    "    \"all\", \"almost\", \"alone\", \"along\", \"already\", \"also\", \"although\", \"always\",\n",
    "    \"am\", \"among\", \"amongst\", \"amoungst\", \"amount\", \"an\", \"and\", \"another\",\n",
    "    \"any\", \"anyhow\", \"anyone\", \"anything\", \"anyway\", \"anywhere\", \"are\",\n",
    "    \"around\", \"as\", \"at\", \"back\", \"be\", \"became\", \"because\", \"become\",\n",
    "    \"becomes\", \"becoming\", \"been\", \"before\", \"beforehand\", \"behind\", \"being\",\n",
    "    \"below\", \"beside\", \"besides\", \"between\", \"beyond\", \"bill\", \"both\",\n",
    "    \"bottom\", \"but\", \"by\", \"call\", \"can\", \"cannot\", \"cant\", \"co\", \"con\",\n",
    "    \"could\", \"couldnt\", \"cry\", \"de\", \"describe\", \"detail\", \"do\", \"done\",\n",
    "    \"down\", \"due\", \"during\", \"each\", \"eg\", \"eight\", \"either\", \"eleven\", \"else\",\n",
    "    \"elsewhere\", \"empty\", \"enough\", \"etc\", \"even\", \"ever\", \"every\", \"everyone\",\n",
    "    \"everything\", \"everywhere\", \"except\", \"few\", \"fifteen\", \"fifty\", \"fill\",\n",
    "    \"find\", \"fire\", \"first\", \"five\", \"for\", \"former\", \"formerly\", \"forty\",\n",
    "    \"found\", \"four\", \"from\", \"front\", \"full\", \"further\", \"get\", \"give\", \"go\",\n",
    "    \"had\", \"has\", \"hasnt\", \"have\", \"he\", \"hence\", \"her\", \"here\", \"hereafter\",\n",
    "    \"hereby\", \"herein\", \"hereupon\", \"hers\", \"herself\", \"him\", \"himself\", \"his\",\n",
    "    \"how\", \"however\", \"hundred\", \"i\", \"ie\", \"if\", \"in\", \"inc\", \"indeed\",\n",
    "    \"interest\", \"into\", \"is\", \"it\", \"its\", \"itself\", \"keep\", \"last\", \"latter\",\n",
    "    \"latterly\", \"least\", \"less\", \"ltd\", \"made\", \"many\", \"may\", \"me\",\n",
    "    \"meanwhile\", \"might\", \"mill\", \"mine\", \"more\", \"moreover\", \"most\", \"mostly\",\n",
    "    \"move\", \"much\", \"must\", \"my\", \"myself\", \"name\", \"namely\", \"neither\",\n",
    "    \"never\", \"nevertheless\", \"next\", \"nine\", \"no\", \"nobody\", \"none\", \"noone\",\n",
    "    \"nor\", \"not\", \"nothing\", \"now\", \"nowhere\", \"of\", \"off\", \"often\", \"on\",\n",
    "    \"once\", \"one\", \"only\", \"onto\", \"or\", \"other\", \"others\", \"otherwise\", \"our\",\n",
    "    \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"part\", \"per\", \"perhaps\",\n",
    "    \"please\", \"put\", \"rather\", \"re\", \"same\", \"see\", \"seem\", \"seemed\",\n",
    "    \"seeming\", \"seems\", \"serious\", \"several\", \"she\", \"should\", \"show\", \"side\",\n",
    "    \"since\", \"sincere\", \"six\", \"sixty\", \"so\", \"some\", \"somehow\", \"someone\",\n",
    "    \"something\", \"sometime\", \"sometimes\", \"somewhere\", \"still\", \"such\",\n",
    "    \"system\", \"take\", \"ten\", \"than\", \"that\", \"the\", \"their\", \"them\",\n",
    "    \"themselves\", \"then\", \"thence\", \"there\", \"thereafter\", \"thereby\",\n",
    "    \"therefore\", \"therein\", \"thereupon\", \"these\", \"they\", \"thick\", \"thin\",\n",
    "    \"third\", \"this\", \"those\", \"though\", \"three\", \"through\", \"throughout\",\n",
    "    \"thru\", \"thus\", \"to\", \"together\", \"too\", \"top\", \"toward\", \"towards\",\n",
    "    \"twelve\", \"twenty\", \"two\", \"un\", \"under\", \"until\", \"up\", \"upon\", \"us\",\n",
    "    \"very\", \"via\", \"was\", \"we\", \"well\", \"were\", \"what\", \"whatever\", \"when\",\n",
    "    \"whence\", \"whenever\", \"where\", \"whereafter\", \"whereas\", \"whereby\",\n",
    "    \"wherein\", \"whereupon\", \"wherever\", \"whether\", \"which\", \"while\", \"whither\",\n",
    "    \"who\", \"whoever\", \"whole\", \"whom\", \"whose\", \"why\", \"will\", \"with\",\n",
    "    \"within\", \"without\", \"would\", \"yet\", \"you\", \"your\", \"yours\", \"yourself\",\n",
    "    \"yourselves\"] #custom stopwords 설정\n",
    "    ##############\n",
    "    tokenized_words_remove=[]\n",
    "    \n",
    "    for w in tokenized_words: \n",
    "        # [5]\n",
    "        if((w in stops)== False): #불용어 집합에 들어있지 않은 경우\n",
    "            tokenized_words_remove.append(w) #불용어가 아닌 리스트에 추가\n",
    "        ##############\n",
    "    \n",
    "    stemmer = PorterStemmer()\n",
    "    for i in range(len(tokenized_words_remove)):\n",
    "        # [6]\n",
    "        stemmer.stem(tokenized_words_remove[i]) #stem 내장 함수로 같은 의미를 가진 단어를 변경하는 과정을 거침\n",
    "        ##############\n",
    "    \n",
    "    return ( \" \".join( tokenized_words_remove ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train=[data_processing(i) for i in X_train]\n",
    "X_test=[data_processing(i) for i in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# [Empty Module #2] 텍스트 데이터 Bag of word  feature  화 \n",
    "# ------------------------------------------------\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# ------------------------------------------------------------\n",
    "# 구현 가이드라인 \n",
    "# ------------------------------------------------------------\n",
    "# [1]  CountVectorizer를 정의 \n",
    "#           1) max_features를 100으로 지정 \n",
    "# [2] X_train 과 X_test를 numpy array로 변환 후 데이터 타입을 \"U\"로 변경해 저장\n",
    "#\n",
    "# [3] CountVectorizer를 이용해 X_train은 학습 및 변환을 하고, X_test는 변환을 진행. \n",
    "# ------------------------------------------------------------\n",
    "###########\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# [1]\n",
    "#vectorizer = CountVectorizer(max_features = 100) #CountVectorizer 정의\n",
    "vectorizer = TfidfVectorizer(max_features = 100)\n",
    "#vectorizer = HashingVectorizer()\n",
    "# [2]\n",
    "X_train = np.array(X_train, dtype = \"U\") #X_train 데이터 타입 및 구조 변환\n",
    "X_test = np.array(X_test, dtype = \"U\") #X_test 데이터 타입 및 구조 변환\n",
    "# [3]\n",
    "X_train_features = vectorizer.fit_transform(X_train) #CountVectorizer로 학습 및 변환\n",
    "X_test_features = vectorizer.transform(X_test) #CountVectorizer로 학습 및 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train[y_train==\"ham\"]=0\n",
    "y_train[y_train==\"spam\"]=1\n",
    "y_train=y_train.astype(\"uint8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------\n",
    "# [Empty Module #3] 텍스트 데이터 Bag of word  feature  화 \n",
    "# ------------------------------------------------\n",
    "from sklearn.svm import SVC\n",
    "# ------------------------------------------------------------\n",
    "# 구현 가이드라인 \n",
    "# ------------------------------------------------------------\n",
    "# [1]  SVC 정의 \n",
    "#           1) gamma=\"auto\" 사용 \n",
    "# [2] X_train_features과 y_train으로 SVC 학습진행 후, X_test_features로 predict 진행\n",
    "#\n",
    "# ------------------------------------------------------------\n",
    "###########\n",
    "# [1]\n",
    "svc=SVC(gamma = 'auto' , probability = True) #SVC 정의\n",
    "# [2]\n",
    "from sklearn.model_selection import GridSearchCV #GridSearchCV 추가 사용\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier\n",
    "rf=RandomForestClassifier(n_estimators = 100)\n",
    "lgb=LGBMClassifier(n_estimators = 100)\n",
    "voting_estimators = [('svc', svc),('rf',rf),('lgb',lgb)]\n",
    "parameters = {'kernel':['rbf','poly', 'sigmoid'], 'C':[0.0001, 0.001, 0.01, 0.1, 1, 10, 50, 75, 100, 125, 1000, 10000]} #파라미터 설정\n",
    "model=GridSearchCV(svc,parameters,cv=10) #model 정의\n",
    "model.fit(X_train_features, y_train) #학습\n",
    "voting = VotingClassifier(estimators = voting_estimators, voting = 'soft')\n",
    "voting.fit(X_train_features.astype('float'), y_train)\n",
    "y_pred2 = voting.predict(X_test_features.astype('float'))\n",
    "y_pred=model.predict(X_test_features) #예측"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred={\"ID\": range(np.array(y_pred2).shape[0]),\"Class\":y_pred2}\n",
    "df_pred=pd.DataFrame(df_pred)\n",
    "df_pred.to_csv(\"predict_voting_hash.csv\",index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=10000, gamma='auto', probability=True)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.best_estimator_ #best_estimator 체크"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
